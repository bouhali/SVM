Q: Move the clusters around to make it easier or harder for the clas-
sifier to find a decent boundary. Pay attention to when the qt
function prints an error message that it can not find a solution.
A:
Ändras klass A&B till att innehålla två linjärt separerbara grupper medför det att linear kernel i princip alltid fungerar.

Q: Implement some of the non-linear kernels. you should be able to
classify very hard datasets.
A:
Sigmoid kernel knappt.

Q: The non-linear kernels have parameters; explore how they influence
the decision boundary. Reason about this in terms of the bias-
variance trade-off.
A:
Polynomial kernel: högre grad ger större varians, kolla seed100/noslack/poly*.png.
Polynomial kernel: när p ökar så ökar komplexiteten av modellen i den mening att fler observationsvektorer blir suportvektorer. Det innebär att fler datapunkter får en avgörande roll i hur klassuppdelningen kommer att se ut. Om fler datapunkter får en avgörande roll kommer uppdelaren vara känslig för ändringar i data dvs hög varians. Angående bias så känns det inte som att det finns en direkt trade-of. Bias verkar snarare bero av formen av delaren och datans ursprung. p=3 i exempelplottarna känns som den delaren med lägst bias. 
Radial kernel: leder låg sigma till låg bias men hög varians, högre sigma blir högre bias lägre varians. Kolla på filerna radialX_slack0_*.png under plots/rand100/noslack.

Q: Explore the role of the parameter C. What happens for very
large/small values?
A:
Polynomial kernel: TODO, svårt att se på plots men borde vara som radial.
Radial kernel: när kostnaden C ökar så minskar bias medan variansen ökar. Notera under plots/seed96_slack/radial15*.png hur bias minskar för större C i och med att svarta linjen passar bättre, medan variansen ökar i och med att röda linjen blir mer detaljerad.
Sammanfattat: Hög C leder till overfitting medan låg C till underfitting.

Q: Imagine that you are given data that is not easily separable. When
should you opt for more slack rather than going for a more complex
model and vice versa?
A:
Om alla datapunkter är en suportvektor kommer modellen inte att generalisera bra, dvs man bör aldrig sträva efter en för komplex modell. Man får hitta en balans mellan bra generalisering och inte för många felklassificeringar. Rent praktiskt kan man addera en kostnadsfunktion för felklassificeringar gjorda pga overfiting. Detta kan implementeras i quadratic pragramming problem. 

